<!doctype html>
<html>
<head>
<!--
	 Global site tag (gtag.js) - Google Analytics 
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-141762792-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-141762792-1');
</script>
-->

<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Omer Bar-Tal's Homepage</title>
<link href="AboutPageAssets/styles/aboutPageStyle.css" rel="stylesheet" type="text/css">
<style type="text/css">
</style>

<!--The following script tag downloads a font from the Adobe Edge Web Fonts server for use within the web page. We recommend that you do not modify it.-->
<script>var __adobewebfontsappname__="dreamweaver"</script><script src="http://use.edgefonts.net/montserrat:n4:default;source-sans-pro:n2:default.js" type="text/javascript"></script>
</head>

<body alink = "#282727" vlink = "#9b9b9b" link = "#9b9b9b">
<!-- Header content -->
<header>
  <div class="profilePhoto"> 
    <!-- Profile photo --> 
    <img src="AboutPageAssets/images/profile1.jpg" alt="sample" width="259"> </div>
<!--	<img src="AboutPageAssets/images/fonitAigolkatan.jpg" alt="sample" width="259"> </div>-->
  <!-- Identity details -->
  <section class="profileHeader">
    <h1>Jisu Nam</h1>
<!--    <h3>M.S./Ph.D. integrated student</h3>-->
    <hr>
    <p>I am a graduate student in the Department of Computer Science and Engineering at Korea University under the supervision of Prof. <a href="https://cvlab.korea.ac.kr/" target="_blank">
		Seungryong Kim</a>. My research is in computer vision and deep learning, I am particularly interested in diffusion-based generative models and visual correspondence.
	<br>
    </p>
  </section>
  <!-- Links to Social network accounts -->
  <aside class="socialNetworkNavBar">
	   <div class="socialNetworkNav"> 
      <!-- Add a Anchor tag with nested img tag here --> 
		   <a href="mailto:jisu_nam@korea.ac.kr">
      <img src="AboutPageAssets/images/mail.png"  alt="sample" width="30" ></a> </div>
      <div class="socialNetworkNav">
      <!-- Add a Anchor tag with nested img tag here --> 
		<a href="https://github.com/jisunam0027" target="_blank">
      <img src="AboutPageAssets/images/github.png" alt="sample" width="30"></a>  </div>
    <div class="socialNetworkNav"> 
		<a href="https://scholar.google.co.kr/citations?hl=ko&user=xakYe8MAAAAJ
" target="_blank">
      <!-- Add a Anchor tag with nested img tag here --> 
      <img src="AboutPageAssets/images/scholar.jpg"  alt="sample" width="30"></a>  </div>
	  	  <div class="socialNetworkNav">
      <!-- Add a Anchor tag with nested img tag here --> 
		<a href="https://www.linkedin.com/in/jisu-nam-9385a4254/" target="_blank">
      <img src="AboutPageAssets/images/linkedin.png" alt="sample" width="30"></a>  </div>
<!--
	  <div class="socialNetworkNav">
		 <a href="https://www.linkedin.com/in/jisu-nam-9385a4254/" target="_blank">
			 <img src="AboutPageAssets/images/linkedin.png"  alt="sample" width="30"> </a></div>
-->
	  
  </aside>
</header>
<!-- content -->
<section class="mainContent"> 
  <!-- Contact details -->
  
  <!-- Previous experience details -->
  
  <section class="section2">
  <h2 class="sectionTitle">Publications</h2>
    <hr class="sectionTitleRule">
    <hr class="sectionTitleRule2">
	  <!-- SMM  -->
	  	<div class="sectionContent">
<!--	 		<img align="center" src="AboutPageAssets/images/teaser-multidiffusion.jpg" style="height: 100%; width: 100%; object-fit: contain"  alt="">-->
			<img src="AboutPageAssets/teasers/dreammatcher_teaser.png" alt="DreamMatcher Teaser Image" width="100%">

	  	</div>
	  	<section class="section2Content">
      		<h2 class="sectionContentTitle"> DreamMatcher: Appearance Matching Self-Attention for Semantically-Consistent Text-to-Image Personalization
      		<h3 class="sectionContentSubTitle"><b>Jisu Nam</b>, <a>Heesu Kim</a>, <a>DongJae Lee</a>, <a>Siyoon Jin</a>, <a>Seungryong Kim</a>, <a>Seunggyu Chang</a> 
	 		</h3>
		  	<h3 class="sectionContentSubTitle"><em>under review at IEEE Conference on Computer Vision Pattern Recognition (CVPR) 2024<b style="color:darksalmon"></b>
	 		</em></h3>
    	</section>
		<aside class="externalResourcesNav">


		  <div class="dropdown"> <span>Abstract</span>
			<div class="dropdown-content">
			  <p style="text-align:left;">The objective of Text-to-Image (T2I) personalization is to customize diffusion models to a user-provided reference concept, generating diverse images of the reference concept aligned with target prompts. Prior methods for T2I personalization utilize the tuning of pre-trained T2I model weights. However, they require a cumbersome tuning process for each concept before generating personalized images. To ease the use of T2I personalization, we propose DreamMatcher, a tuning-free plug-in method that provides semantically aligned visual conditions of the reference concept to a diffusion model during the denoising process. We find that the key and value in the self-attention module play important roles in generating the structure and appearance of a personalized image, respectively. Hence, DreamMatcher replaces the target values with reference values aligned by semantic matching, leaving the structure path unchanged to preserve the versatile capability of pre-trained T2I models for generating diverse structures. We also employ a semantic-consistent masking strategy to isolate regions irrelevant to the personalized concept, such as the background, from core regions that should be replaced by the aligned appearances. DreamMatcher is compatible with existing T2I models, showing significant improvements in complex, non-rigid scenarios. Intensive experiments demonstrate the effectiveness and superiority of our approach compared to other baselines.	  </p></div>
		  </div>
		 </aside>

	  <br><br>

	  <!-- TokenFlow  -->
	  	<div class="sectionContent">
<!--	 		<img align="center" src="AboutPageAssets/images/teaser-multidiffusion.jpg" style="height: 100%; width: 100%; object-fit: contain"  alt="">-->
<img src="AboutPageAssets/teasers/diffmatch_teaser.png" alt="DiffMatch Teaser Image" width="100%">
	  	</div>
	  	<section class="section2Content">
      		<h2 class="sectionContentTitle"> Diffusion Model for Dense Matching</h2>
      		<h3 class="sectionContentSubTitle"><b>Jisu Nam</b>, <a>Gyuseong Lee</a>, <a>Sunwoo Kim</a>, <a>Hyeonsu Kim,</a>, <a>Hyoungwon Cho</a>, <a>Seyeon Kim</a>, <a>Seungryong
Kim</a>  
	 		</h3>
		  	<h3 class="sectionContentSubTitle"><em>International Conference on Learning Representations (ICLR) 2024 , <b style="color:darksalmon">Oral, 1.2% acceptance rate</b>
	 		</em></h3>
    	</section>
		<aside class="externalResourcesNav">

		  <div class="dropdown"><span></span><a href="https://ku-cvlab.github.io/DiffMatch/" target="_blank">Project Page</a></div>
		  <div class="dropdown"> <span>Abstract</span>
			<div class="dropdown-content">
			  <p style="text-align:left;">The objective for establishing dense correspondence between paired images con- sists of two terms: a data term and a prior term. While conventional techniques focused on defining hand-designed prior terms, which are difficult to formulate, recent approaches have focused on learning the data term with deep neural net- works without explicitly modeling the prior, assuming that the model itself has the capacity to learn an optimal prior from a large-scale dataset. The performance improvement was obvious, however, they often fail to address inherent ambiguities of matching, such as textureless regions, repetitive patterns, large displacements, or noises. To address this, we propose DiffMatch, a novel conditional diffusion-based framework designed to explicitly model both the data and prior terms for dense matching. This is accomplished by leveraging a conditional denoising diffusion model that explicitly takes matching cost and injects the prior within generative process. However, limited resolution of the diffusion model is a major hindrance. We address this with a cascaded pipeline, starting with a low-resolution model, followed by a super-resolution model that successively upsamples and incorporates finer details to the matching field. Our experimental results demonstrate signifi- cant performance improvements of our method over existing approaches, and the ablation studies validate our design choices along with the effectiveness of each component. The code and pretrained weights will be available.			  </p></div>
		  </div>
		  <div class="dropdown"><span></span><a href="" target="https://openreview.net/forum?id=Zsfiqpft6K">Paper</a></div>
		  <div class="dropdown"><span></span><a href="https://github.com/KU-CVLAB/DiffMatch" target="_blank">Code</a></div>
		</aside>

	  <br><br>

	  <!-- MultiDiffuse  -->
	  	<div class="sectionContent">
<!--	 		<img align="center" src="AboutPageAssets/images/teaser-multidiffusion.jpg" style="height: 100%; width: 100%; object-fit: contain"  alt="">-->
			<img src="AboutPageAssets/teasers/diffface_teaser.png" alt="DiffFace Teaser Image" width="100%">
	  	</div>
	  	<section class="section2Content">
      		<h2 class="sectionContentTitle"> DiffFace: Diffusion-based Face Swapping with Facial Guidance</h2>
      		<h3 class="sectionContentSubTitle">Kihong Kim, Yunho Kim, Seokju Cho, Junyoung Seo, <b>Jisu Nam</b>, Kychul Lee, Seungryong
Kim and KwangHee Lee </h3>
		  	<h3 class="sectionContentSubTitle"><em>arXiv, 2022<b style="color:darksalmon"></b>
	 		</em></h3>
    	</section>
		<aside class="externalResourcesNav">

		  <div class="dropdown"><span></span><a href="https://hxngiee.github.io/DiffFace/" target="_blank">Project Page</a></div>
		  <div class="dropdown"> <span>Abstract</span>
			<div class="dropdown-content">
			  <p style="text-align:left;">In this paper, we propose a novel diffusion-based face swapping framework, called DiffFace, composed of training ID Conditional DDPM, sampling with facial guidance, and a target-preserving blending. In specific, in the training process, the ID Conditional DDPM is trained to generate face images with the desired identity. In the sampling process, we use the off-the-shelf facial expert models to make the model transfer source identity while preserving target attributes faithfully. During this process, to preserve the background of the target image, we additionally propose a target-preserving blending strategy. It helps our model to keep the attributes of the target face from noise while transferring the source facial identity. In addition, without any re-training, our model can flexibly apply additional facial guidance and adaptively control the ID-attributes trade-off to achieve the desired results. To the best of our knowledge, this is the first approach that applies the diffusion model in face swapping task. Compared with previous GAN-based approaches, by taking advantage of the diffusion model for the face swapping task, DiffFace achieves better benefits such as training stability, high fidelity, and controllability. Extensive experiments show that our DiffFace is comparable or superior to the state-of-the-art methods on the standard face swapping benchmark.		  </p>
			</div>
		  </div>
		  <div class="dropdown"><span></span><a href="" target="https://arxiv.org/abs/2212.13344">Paper</a></div>
		  <div class="dropdown"><span></span><a href="https://github.com/hxngiee/DiffFace" target="_blank">Code</a></div>
		</aside>

	  <br><br>

    	<!-- Text2LIVE  -->
	  	<div class="sectionContent">
<!--	 		<img align="center" src="AboutPageAssets/images/volsdf_2.gif" style="height: 100%; width: 100%; object-fit: contain"  alt="">-->
		<img src="AboutPageAssets/teasers/nemf_teaser.png" alt="nemf Teaser Image" width="100%">
	  	</div>
	  	<section class="section2Content">
      		<h2 class="sectionContentTitle"> Neural Matching Fields: Implicit Representation of Matching Fields for Visual Correspondence</h2>
      		<h3 class="sectionContentSubTitle">Sunghwan Hong, <b>Jisu Nam</b>, Seokju Cho, Susung Hong, Sangryul Jeon, Dongbo Min, and Seungryong
Kim  </h3>
		  	<h3 class="sectionContentSubTitle"><em>Neural Information Processing Systems (NeurIPS) 2022
	 		</em></h3>
    	</section>
		<aside class="externalResourcesNav"> 
		
		  <div class="dropdown"><span></span><a href="https://ku-cvlab.github.io/NeMF/" target="_blank">Project Page</a></div>
		  <div class="dropdown"> <span>Abstract</span>
			<div class="dropdown-content">
			  <p style="text-align:left;">
				Existing pipelines of semantic correspondence commonly include extracting high level semantic features for the invariance against intra-class variations and background clutters. This architecture, however, inevitably results in a low-resolution matching field that additionally requires an ad-hoc interpolation process as a post-processing for converting it into a high-resolution one, certainly limiting the performance of matching results. To overcome this, inspired by recent success of implicit neural representation, we present a novel method for semantic correspondence, called neural matching field (NeMF). However, complicacy and high-dimensionality of a 4D matching field are the major hindrances. To address them, we propose a cost embedding network consisting of convolution and self-attention layers to process the coarse cost volume to obtain cost feature representation, which is used as a guidance for establishing high-precision matching field through the following fully-connected network. Although this may help to better structure the matching field, learning a high-dimensional matching field remains challenging mainly due to computational complexity, since a naïve ex- haustive inference would require querying from all pixels in the 4D space to infer pixel-wise correspondences. To overcome this, in the training phase, we randomly sample matching candidates. In the inference phase, we propose a novel inference approach which iteratively performs PatchMatch-based inference and coordinate optimization at test time. With the proposed method, competitive results are at- tained on several standard benchmarks for semantic correspondence.		  </p>
			</div>
		  </div>
		  <div class="dropdown"><span></span><a href="https://arxiv.org/abs/2210.02689" target="_blank">Paper</a></div>
		  <div class="dropdown"><span></span><a href="https://github.com/KU-CVLAB/NeMF" target="_blank">Code</a></div>
		</aside>
	  
		<br><br>
		
	  <!-- Text2LIVE  -->
	  <div class="sectionContent">
<!--	 		<img align="center" src="AboutPageAssets/images/volsdf_2.gif" style="height: 100%; width: 100%; object-fit: contain"  alt="">-->
		<img src="AboutPageAssets/teasers/vat_teaser.png" alt="vat Teaser Image"  width="100%">
	  	</div>
	  	<section class="section2Content">
      		<h2 class="sectionContentTitle"> Cost Aggregation with 4D Convolutional Swin Transformer for Few-Shot Segmentation</h2>
			  <h3 class="sectionContentSubTitle">Sunghwan Hong*, Seokju Cho*, <b>Jisu Nam</b>, Stephen Lin, Seungryong Kim</h3>
		  	<h3 class="sectionContentSubTitle"><em>European Conference on Computer Vision (ECCV), 2022
	 		</em></h3>
    	</section>
		<aside class="externalResourcesNav">

		  <div class="dropdown"><span></span><a href="https://seokju-cho.github.io/VAT/" target="_blank">Project Page</a></div>
		  <div class="dropdown"> <span>Abstract</span>
			<div class="dropdown-content">
			  <p style="text-align:left;">
 			 This paper presents a novel cost aggregation network, called Volumetric Aggregation with Transformers (VAT), for few-shot segmentation. The use of transformers can benefit correlation map aggregation through self-attention over a global receptive field. However, the tokenization of a correlation map for transformer processing can be detrimental, because the discontinuity at token boundaries reduces the local context available near the token edges and decreases inductive bias. To address this problem, we propose a 4D Convolutional Swin Transformer, where a high-dimensional Swin Transformer is preceded by a series of small-kernel convolutions that impart local context to all pixels and introduce convolutional inductive bias. We additionally boost aggregation performance by applying transformers within a pyramidal structure, where aggregation at a coarser level guides aggregation at a finer level. Noise in the transformer output is then filtered in the subsequent decoder with the help of the query’s appearance embedding. With this model, a new state-of-the-art is set for all the standard benchmarks in few-shot segmentation. It is shown that VAT attains state-of-the-art performance for semantic correspondence as well, where cost aggregation also plays a central role.
			  </p>
			</div>
		  </div>
		  <div class="dropdown"><span></span><a href="https://splice-vit.github.io/#paper" target="_blank">Paper</a></div>
		  <div class="dropdown"><span></span><a href="https://github.com/Seokju-Cho/Volumetric-Aggregation-Transformer" target="_blank">Code</a></div>
		</aside>
	  
	  <br><br>

  <p class="footerDisclaimer">Template of <a href="https://lioryariv.github.io/">Lior Yariv</a><span></span></p>
  <p class="footerNote"></p>
</footer>
</body>
</html>
