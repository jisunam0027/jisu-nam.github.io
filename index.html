<!doctype html>
<html>
<head>
<!--
	 Global site tag (gtag.js) - Google Analytics 
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-141762792-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-141762792-1');
</script>
-->

<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Jisu Nam's Homepage</title>
<link href="AboutPageAssets/styles/aboutPageStyle.css" rel="stylesheet" type="text/css">
<style type="text/css">
</style>

<!--The following script tag downloads a font from the Adobe Edge Web Fonts server for use within the web page. We recommend that you do not modify it.-->
<script>var __adobewebfontsappname__="dreamweaver"</script><script src="http://use.edgefonts.net/montserrat:n4:default;source-sans-pro:n2:default.js" type="text/javascript"></script>
</head>

<body alink = "#282727" vlink = "#9b9b9b" link = "#9b9b9b">
<!-- Header content -->
<header>
  <div class="profilePhoto"> 
    <!-- Profile photo --> 
    <img src="AboutPageAssets/images/profile1.jpg" alt="sample" width="259"> </div>
<!--	<img src="AboutPageAssets/images/fonitAigolkatan.jpg" alt="sample" width="259"> </div>-->
  <!-- Identity details -->
  <section class="profileHeader">
    <h1>Jisu Nam</h1>
<!--    <h3>M.S./Ph.D. integrated student</h3>-->
    <hr>
    I am a 3rd year Ph.D. student at <a href="https://cvlab.kaist.ac.kr/" target="_blank">Computer Vision LAB (CVLAB)</a>, advised by Prof. <a href="https://cvlab.kaist.ac.kr/members/faculty" target="_blank">Seungryong Kim</a> at at <a href="https://www.kaist.ac.kr/en/" target="_blank">Korea Advanced Institute of Science and Technology (KAIST)</a>.
	  My research interests include Computer Vision, Machine Learning, and Deep Learning, particularly Text-to-Image/Video Customization and Image Matching. For more information, please refer to my <a href="AboutPageAssets/images/cv.pdf" target="_blank">CV</a>.<br>
	<br>
  </section>
  <!-- Links to Social network accounts -->
  <aside class="socialNetworkNavBar">
	   <div class="socialNetworkNav"> 
      <!-- Add a Anchor tag with nested img tag here --> 
		   <a href="mailto:jisunam@kaist.ac.kr">
      <img src="AboutPageAssets/images/mail.png"  alt="sample" width="30" ></a> </div>
      <div class="socialNetworkNav">
      <!-- Add a Anchor tag with nested img tag here --> 
		<a href="https://github.com/nam-jisu" target="_blank">
      <img src="AboutPageAssets/images/github.png" alt="sample" width="30"></a>  </div>
    <div class="socialNetworkNav"> 
		<a href="https://scholar.google.co.kr/citations?hl=ko&user=xakYe8MAAAAJ
" target="_blank">
      <!-- Add a Anchor tag with nested img tag here --> 
      <img src="AboutPageAssets/images/scholar.jpg"  alt="sample" width="30"></a>  </div>
	  	  <div class="socialNetworkNav">
      <!-- Add a Anchor tag with nested img tag here --> 
		<a href="https://www.linkedin.com/in/jisu-nam-9385a4254/" target="_blank">
      <img src="AboutPageAssets/images/linkedin.png" alt="sample" width="30"></a>  </div>
<!--
	  <div class="socialNetworkNav">
		 <a href="https://www.linkedin.com/in/jisu-nam-9385a4254/" target="_blank">
			 <img src="AboutPageAssets/images/linkedin.png"  alt="sample" width="30"> </a></div>
-->
	  
  </aside>
</header>
<!-- content -->
<section class="mainContent"> 
  <!-- Contact details -->
  
  <section class="section1">
	<h2 class="sectionTitle">Work Experience</h2>
	<hr class="sectionRule">
	<br>
	<b>Research Intern at Adobe, San Jose, CA</b> (2024.05 - 2024.08)</h2> <br>
	Mentors: <a href "https://research.adobe.com/person/yang-zhou/"  target="_blank"> Yang Zhou</a>, <a href "https://research.adobe.com/person/feng-liu/"  target="_blank"> Feng Liu</a>, <a href "https://research.adobe.com/person/jing-shi/"  target="_blank"> Jing Shi</a>, <a href "https://research.adobe.com/person/difan-liu/"  target="_blank"> Difan Liu</a>, <a href "https://scholar.google.com/citations?user=pF2vMhgAAAAJ&hl=zh-CN"  target="_blank"> Zhan Xu</a> <br>
	Project: Encoder-based Consistent Human Character Customization <br>
	<br>
	<b>Research Intern at Naver Cloud, Seoul, Korea</b> (2023.04 - 2023.10)</h2> <br>
	Mentors: Seunggyu Chang, Heesu Kim, DongJae Lee <br>
	Project: Training-free Semantically Consistent Text-to-Image Customization
	<br>
	<br>

	

  <!-- Previous experience details -->
  <section class="section2">
  <h2 class="sectionTitle">Publications</h2>
    <hr class="sectionTitleRule">
    <hr class="sectionTitleRule2">

	<!-- LocoTrack  -->
	  	<div class="sectionContent">
<img src="AboutPageAssets/teasers/locotrack_teaser.png" alt="Locotrack Teaser Image" width="100%">
	  	</div>
	  	<section class="section2Content">
			<br>
      		<h2 class="sectionContentTitle"> Local All-Pair Correspondence for Point Tracking</h2>
      		<h3 class="sectionContentSubTitle"><a>Seokju Cho</a>, <a>Jiahui Huang</a>, <b>Jisu Nam</b>, <a>Honggyu An</a>, <a>Seungryong Kim<sup>&dagger;</sup></a>, <a>Joon-Young Lee<sup>&dagger;</sup></a>
	 		</h3>
		  	<h3 class="sectionContentSubTitle"><em> ECCV 2024
	 		</em></h3>
    	</section>
		<aside class="externalResourcesNav">

		  <div class="dropdown"><span></span><a href="https://ku-cvlab.github.io/locotrack/" target="_blank">Project Page</a></div>
		  <div class="dropdown"> <span>Abstract</span>
			<div class="dropdown-content">
			  <p style="text-align:left;">We introduce LocoTrack, a highly accurate and efficient model designed for the task of tracking any point (TAP) across video sequences. Previous approaches in this task often rely on local 2D correlation maps to establish correspondences from a point in the query image to a local region in the target image, which often struggle with homogeneous regions or repetitive features, leading to matching ambiguities. LocoTrack overcomes this challenge with a novel approach that utilizes all-pair correspondences across regions, i.e., local 4D correlation, to establish precise correspondences, with bidirectional correspondence and matching smoothness significantly enhancing robustness against ambiguities. We also incorporate a lightweight correlation encoder to enhance computational efficiency, and a compact Transformer architecture to integrate long-term temporal information. LocoTrack achieves unmatched accuracy on all TAP-Vid benchmarks and operates at a speed almost 6 times faster than the current state-of-the-art.			  </p></div>
		  </div>
		  <div class="dropdown"><span></span><a href="https://arxiv.org/abs/2407.15420" target="_blank">Paper</a></div>
		  <div class="dropdown"><span></span><a href="https://github.com/KU-CVLAB/LocoTrack" target="_blank">Code</a></div>
		</aside>

	  <br><br>

	<!-- DreamMatcher  -->
	  	<div class="sectionContent">
<img src="AboutPageAssets/teasers/dreammatcher_teaser.png" alt="DreamMatcher Teaser Image" width="100%">
	  	</div>
	  	<section class="section2Content">
      		<h2 class="sectionContentTitle"> DreamMatcher: Appearance Matching Self-Attention for Semantically-Consistent Text-to-Image Personalization</h2>
      		<h3 class="sectionContentSubTitle"><b>Jisu Nam</b>, <a>Heesu Kim</a>, <a>DongJae Lee</a>, <a>Siyoon Jin</a>, <a>Seungryong Kim<sup>&dagger;</sup></a>, <a>Seunggyu Chang<sup>&dagger;</sup></a> 
	 		</h3>
		  	<h3 class="sectionContentSubTitle"><em> CVPR 2024
	 		</em></h3>
    	</section>
		<aside class="externalResourcesNav">

		  <div class="dropdown"><span></span><a href="https://ku-cvlab.github.io/DreamMatcher/" target="_blank">Project Page</a></div>
		  <div class="dropdown"> <span>Abstract</span>
			<div class="dropdown-content">
			  <p style="text-align:left;">The objective of Text-to-Image (T2I) personalization is to customize a diffusion model to a user-provided reference concept, generating diverse images of the concept aligned with the target prompts. Conventional methods for T2I personalization represent the reference concepts using unique text embeddings. However, they often fail to accurately mimic the appearance of the reference. To address this, one solution may be explicitly conditioning the reference images into the target denoising process, called key-value replacement. However, prior works are constrained to local editing since they disrupt the structure path of the pre-trained T2I model, leading to suboptimal matching. To overcome this, we propose a novel plug-in method, DreamMatcher, which reformulates T2I personalization as semantic matching. Specifically, DreamMatcher replaces the target values with reference values aligned by semantic matching, while leaving the structure path unchanged to preserve the versatile capability of pre-trained T2I models for generating diverse structures. We also introduce a semantic-consistent masking strategy to isolate the personalized concept from irrelevant regions, such as the background or objects newly introduced by the target prompts. DreamMatcher is compatible with any existing T2I models, showing significant improvements in complex personalization scenarios. Intensive experiments and analyses demonstrate the effectiveness of our approach.			  </p></div>
		  </div>
		  <div class="dropdown"><span></span><a href="https://arxiv.org/abs/2402.09812" target="_blank">Paper</a></div>
		  <div class="dropdown"><span></span><a href="https://github.com/KU-CVLAB/DreamMatcher" target="_blank">Code</a></div>
		</aside>

	  <br><br>

	  <!-- DiffMatch  -->
	  	<div class="sectionContent">
<img src="AboutPageAssets/teasers/diffmatch_teaser.png" alt="DiffMatch Teaser Image" width="100%">
	  	</div>
	  	<section class="section2Content">
      		<h2 class="sectionContentTitle"> Diffusion Model for Dense Matching</h2>
      		<h3 class="sectionContentSubTitle"><b>Jisu Nam</b>, <a>Gyuseong Lee</a>, <a>Sunwoo Kim</a>, <a>Hyeonsu Kim</a>, <a>Hyoungwon Cho</a>, <a>Seyeon Kim</a>, <a>Seungryong
Kim</a>  
	 		</h3>
		  	<h3 class="sectionContentSubTitle"><em>ICLR 2024, <b style="color:darksalmon">Oral, 1.2% acceptance rate</b>
	 		</em></h3>
    	</section>
		<aside class="externalResourcesNav">

		  <div class="dropdown"><span></span><a href="https://ku-cvlab.github.io/DiffMatch/" target="_blank">Project Page</a></div>
		  <div class="dropdown"> <span>Abstract</span>
			<div class="dropdown-content">
			  <p style="text-align:left;">The objective for establishing dense correspondence between paired images conists of two terms: a data term and a prior term. While conventional techniques focused on defining hand-designed prior terms, which are difficult to formulate, recent approaches have focused on learning the data term with deep neural net- works without explicitly modeling the prior, assuming that the model itself has the capacity to learn an optimal prior from a large-scale dataset. The performance improvement was obvious, however, they often fail to address inherent ambiguities of matching, such as textureless regions, repetitive patterns, large displacements, or noises. To address this, we propose DiffMatch, a novel conditional diffusion-based framework designed to explicitly model both the data and prior terms for dense matching. This is accomplished by leveraging a conditional denoising diffusion model that explicitly takes matching cost and injects the prior within generative process. However, limited resolution of the diffusion model is a major hindrance. We address this with a cascaded pipeline, starting with a low-resolution model, followed by a super-resolution model that successively upsamples and incorporates finer details to the matching field. Our experimental results demonstrate signifi- cant performance improvements of our method over existing approaches, and the ablation studies validate our design choices along with the effectiveness of each component. The code and pretrained weights will be available.			  </p></div>
		  </div>
		  <div class="dropdown"><span></span><a href="https://openreview.net/pdf/969f922ad6124503cc67cacd76aa7e7b928a7882.pdf" target="_blank">Paper</a></div>
		  <div class="dropdown"><span></span><a href="https://github.com/KU-CVLAB/DiffMatch" target="_blank">Code</a></div>
		</aside>

	  <br><br>

    	<!-- NeMF  -->
	  	<div class="sectionContent">
		<img src="AboutPageAssets/teasers/nemf_teaser.png" alt="nemf Teaser Image" width="100%">
	  	</div>
	  	<section class="section2Content">
      		<h2 class="sectionContentTitle"> Neural Matching Fields: Implicit Representation of Matching Fields for Visual Correspondence</h2>
      		<h3 class="sectionContentSubTitle">Sunghwan Hong, <b>Jisu Nam</b>, Seokju Cho, Susung Hong, Sangryul Jeon, Dongbo Min, and Seungryong
Kim  </h3>
		  	<h3 class="sectionContentSubTitle"><em> NeurIPS 2022
	 		</em></h3>
    	</section>
		<aside class="externalResourcesNav"> 
		
		  <div class="dropdown"><span></span><a href="https://ku-cvlab.github.io/NeMF/" target="_blank">Project Page</a></div>
		  <div class="dropdown"> <span>Abstract</span>
			<div class="dropdown-content">
			  <p style="text-align:left;">
				Existing pipelines of semantic correspondence commonly include extracting high level semantic features for the invariance against intra-class variations and background clutters. This architecture, however, inevitably results in a low-resolution matching field that additionally requires an ad-hoc interpolation process as a post-processing for converting it into a high-resolution one, certainly limiting the performance of matching results. To overcome this, inspired by recent success of implicit neural representation, we present a novel method for semantic correspondence, called neural matching field (NeMF). However, complicacy and high-dimensionality of a 4D matching field are the major hindrances. To address them, we propose a cost embedding network consisting of convolution and self-attention layers to process the coarse cost volume to obtain cost feature representation, which is used as a guidance for establishing high-precision matching field through the following fully-connected network. Although this may help to better structure the matching field, learning a high-dimensional matching field remains challenging mainly due to computational complexity, since a naïve ex- haustive inference would require querying from all pixels in the 4D space to infer pixel-wise correspondences. To overcome this, in the training phase, we randomly sample matching candidates. In the inference phase, we propose a novel inference approach which iteratively performs PatchMatch-based inference and coordinate optimization at test time. With the proposed method, competitive results are at- tained on several standard benchmarks for semantic correspondence.		  </p>
			</div>
		  </div>
		  <div class="dropdown"><span></span><a href="https://arxiv.org/abs/2210.02689" target="_blank">Paper</a></div>
		  <div class="dropdown"><span></span><a href="https://github.com/KU-CVLAB/NeMF" target="_blank">Code</a></div>
		</aside>
	  
		<br><br>
		
	  <!--VAT  -->
	  <div class="sectionContent">
		<img src="AboutPageAssets/teasers/vat_teaser.png" alt="vat Teaser Image"  width="100%">
	  	</div>
	  	<section class="section2Content">
      		<h2 class="sectionContentTitle"> Cost Aggregation with 4D Convolutional Swin Transformer for Few-Shot Segmentation</h2>
			  <h3 class="sectionContentSubTitle">Sunghwan Hong*, Seokju Cho*, <b>Jisu Nam</b>, Stephen Lin, Seungryong Kim</h3>
		  	<h3 class="sectionContentSubTitle"><em>ECCV 2022
	 		</em></h3>
    	</section>
		<aside class="externalResourcesNav">

		  <div class="dropdown"><span></span><a href="https://seokju-cho.github.io/VAT/" target="_blank">Project Page</a></div>
		  <div class="dropdown"> <span>Abstract</span>
			<div class="dropdown-content">
			  <p style="text-align:left;">
 			 This paper presents a novel cost aggregation network, called Volumetric Aggregation with Transformers (VAT), for few-shot segmentation. The use of transformers can benefit correlation map aggregation through self-attention over a global receptive field. However, the tokenization of a correlation map for transformer processing can be detrimental, because the discontinuity at token boundaries reduces the local context available near the token edges and decreases inductive bias. To address this problem, we propose a 4D Convolutional Swin Transformer, where a high-dimensional Swin Transformer is preceded by a series of small-kernel convolutions that impart local context to all pixels and introduce convolutional inductive bias. We additionally boost aggregation performance by applying transformers within a pyramidal structure, where aggregation at a coarser level guides aggregation at a finer level. Noise in the transformer output is then filtered in the subsequent decoder with the help of the query’s appearance embedding. With this model, a new state-of-the-art is set for all the standard benchmarks in few-shot segmentation. It is shown that VAT attains state-of-the-art performance for semantic correspondence as well, where cost aggregation also plays a central role.
			  </p>
			</div>
		  </div>
		  <div class="dropdown"><span></span><a href="https://arxiv.org/abs/2207.10866" target="_blank">Paper</a></div>
		  <div class="dropdown"><span></span><a href="https://github.com/Seokju-Cho/Volumetric-Aggregation-Transformer" target="_blank">Code</a></div>
		</aside>
	  
	  <br><br>

  <p class="footerDisclaimer">Template of <a href="https://lioryariv.github.io/">Lior Yariv</a><span></span></p>
  <p class="footerNote"></p>
</footer>
</body>
</html>
