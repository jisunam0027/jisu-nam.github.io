<!doctype html>
<html>

<!-- Google Fonts -->
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Nunito:wght@200..1000&family=Open+Sans:wght@300..800&family=Roboto:wght@100..900&family=Winky+Sans:wght@300..900&display=swap" rel="stylesheet">

<style>
  /* ====== Font Styling ====== */
  body, h1, h2, h3, h4, h5, h6, p, a, span, div, li, button, input, textarea, section, footerNote, footerDisclaimer {
    font-family: 'Nunito', sans-serif !important;
  }

  /* ====== Font Sizes ====== */
  .profileHeader { font-size: 16px !important; } /* Introduction */
  .sectionTitle { font-size: 20px !important; }  /* Section Title */
  .section1 { font-size: 17px; } /* Work Experience */
  .sectionContentTitle { font-size: 18px !important; } /* Paper Title */
  .sectionContentSubTitle { font-size: 17px !important; } /* Author Names */
  .dropdown { font-size: 17px !important; }  /* Gray Box */

  /* ====== Remove Space Between Paper Name and Author Names ====== */
  .sectionContentTitle { margin-bottom: 0 !important; }
  .sectionContentSubTitle { margin-top: 0 !important; }

  /* ====== Image Centering and Scaling ====== */

  .sectionContent img { 
    width: 100% !important; 
    height: auto !important;
	justify-content: left !important;
    align-items: left !important;
  }
  
  /* ====== Profile and Text ====== */
  .profileHeader {
		margin-left: 300px !important; /* Adds space between the image and text */
	}

	.adobe-logo {
		width: 18px; /* Adjust the logo size here (smaller value) */
		vertical-align: middle; /* Align logo with the text */
		margin-right: 2px; /* Space between the logo and text */
	}

	.naver-logo {
		width: 18px; /* Adjust the logo size here (smaller value) */
		vertical-align: middle; /* Align logo with the text */
		margin-right: 2px; /* Space between the logo and text */
	}

</style>


  

<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Jisu Nam's Homepage</title>
<link href="AboutPageAssets/styles/aboutPageStyle.css" rel="stylesheet" type="text/css">
<style type="text/css">
</style>

<!--The following script tag downloads a font from the Adobe Edge Web Fonts server for use within the web page. We recommend that you do not modify it.-->
<script>var __adobewebfontsappname__="dreamweaver"</script><script src="http://use.edgefonts.net/montserrat:n4:default;source-sans-pro:n2:default.js" type="text/javascript"></script>
</head>

<body alink = "#282727" vlink = "#9b9b9b" link = "#9b9b9b">
	
<!-- Header content -->
<header>
  <div class="profilePhoto"> 
    <!-- Profile photo --> 
	<br>
	<br>
	
    <img src="AboutPageAssets/images/profile1.jpg" alt="sample" width="263"> </div>

  <!-- Identity details -->
  <section class="profileHeader">
	
    <h1>Jisu Nam</h1>
<!--    <h3>M.S./Ph.D. integrated student</h3>-->
    <hr>
	I am a third-year Ph.D. student at KAIST, advised by <a href="https://cvlab.kaist.ac.kr/" target="_blank">Seungryong Kim</a>. I am closely working with <a href="https://scholar.google.com/citations?user=UuwugFEAAAAJ&hl=en"  target="_blank"> Yang Zhou</a> at Adobe Research through a couple of internships and collaborating with <a href="https://scholar.google.com/citations?user=z4dNJdkAAAAJ&hl=ko"  target="_blank"> Junhwa Hur</a> at Google DeepMind.
	<br>
	<br>
	

	My research focuses on general perception for images and videos, particularly correspondence in images and videos (image matching and point tracking). Recently, I have been working on large-scale image/video generative models, analyzing their learned representations in the context of correspondence—ultimately to understand, enhance, and manipulate the synthesis process.

	<br>
	<br>
	I’d love to connect with people who share similar interests! Feel free to reach out via email!
  </section>
  

  <!-- Links to Social network accounts -->
  <aside class="socialNetworkNavBar">
	<div class="socialNetworkNav">
      <!-- Add a Anchor tag with nested img tag here --> 
		<a href="AboutPageAssets/images/cv.pdf" target="_blank">
      <img src="AboutPageAssets/images/cv.png" alt="sample" width="30"></a>  </div>
	   <div class="socialNetworkNav"> 
      <!-- Add a Anchor tag with nested img tag here --> 
		   <a href="mailto:jisunam@kaist.ac.kr">
      <img src="AboutPageAssets/images/mail.png"  alt="sample" width="30" ></a> </div>
    <div class="socialNetworkNav"> 
		<a href="https://scholar.google.co.kr/citations?hl=ko&user=xakYe8MAAAAJ
" target="_blank">
      <!-- Add a Anchor tag with nested img tag here --> 
      <img src="AboutPageAssets/images/scholar.jpg"  alt="sample" width="30"></a>  </div>
	  	  <div class="socialNetworkNav">
      <!-- Add a Anchor tag with nested img tag here --> 
		<a href="https://www.linkedin.com/in/jisu-nam-9385a4254/" target="_blank">
      <img src="AboutPageAssets/images/linkedin.png" alt="sample" width="30"></a>  </div>
<!--
	  <div class="socialNetworkNav">
		 <a href="https://www.linkedin.com/in/jisu-nam-9385a4254/" target="_blank">
			 <img src="AboutPageAssets/images/linkedin.png"  alt="sample" width="30"> </a></div>
-->
<div class="socialNetworkNav">
      <!-- Add a Anchor tag with nested img tag here --> 
		<a href="https://github.com/nam-jisu" target="_blank">
      <img src="AboutPageAssets/images/github.png" alt="sample" width="30"></a>  </div>
	  
  </aside>
</header>
<!-- content -->
<section class="mainContent"> 
  <!-- Contact details -->
  
  <br><br>

  <section class="section1">
	<h2 class="sectionTitle">Work Experience</h2>
	<hr class="sectionRule">

	<div class="internship">
        <img src="AboutPageAssets/teasers/adobe_teaser.png" alt="Adobe Logo" class="adobe-logo">
        <b>Research Intern at Adobe Research</b>, San Jose, CA (2025.05 - 2025.08, Expected) <br>
        Mentors: <a href="https://research.adobe.com/person/yang-zhou/" target="_blank">Yang Zhou</a> <br>
    </div>

	<br>

	<div class="internship">
        <img src="AboutPageAssets/teasers/adobe_teaser.png" alt="Adobe Logo" class="adobe-logo">
        <b>Research Intern at Adobe Research</b>, San Jose, CA (2024.05 - 2024.08) <br>
        Mentors: <a href="https://research.adobe.com/person/yang-zhou/" target="_blank">Yang Zhou</a>, 
        <a href="https://research.adobe.com/person/feng-liu/" target="_blank">Feng Liu</a>, 
        <a href="https://research.adobe.com/person/jing-shi/" target="_blank">Jing Shi</a>, 
        <a href="https://research.adobe.com/person/difan-liu/" target="_blank">Difan Liu</a>, 
        <a href="https://scholar.google.com/citations?user=pF2vMhgAAAAJ&hl=zh-CN" target="_blank">Zhan Xu</a> <br>
        Project: Foundation Model for Consistent Full-Body Human Generation <br>
    </div>

    <br>

    <div class="internship">
		<img src="AboutPageAssets/teasers/naver_teaser.png" alt="NAVER Logo" class="naver-logo">
        <b>Research Intern at Naver Cloud</b>, Seoul, Korea (2023.04 - 2023.10) <br>
        Mentors: Seunggyu Chang, Heesu Kim, DongJae Lee <br>
        Project: Training-free Semantically Consistent Image Generation
    </div>

<br>
<br>

  <!-- Previous experience details -->
  <section class="section2">
  <h2 class="sectionTitle">Selected Publications</h2>
    <hr class="sectionTitleRule">
    <hr class="sectionTitleRule2">

	<!-- Visual Persona  -->
	  	<div class="sectionContent">
<img src="AboutPageAssets/teasers/visual_persona_teaser.png" alt="Visual Persona Teaser Image" width="100%">
	  	</div>
	  	<section class="section2Content">
      		<h2 class="sectionContentTitle"> Visual Persona: Foundation Model for Full-Body Human Customization</h2>
      		<h3 class="sectionContentSubTitle"><b>Jisu Nam</b>, <a>Soowon Son</a>, <a>Zhan Xu</a>, <a>Jing Shi</a>, <a>Difan Liu</a>, <a>Feng Liu</a>, <a>Aashish Misraa</a>, <a>Seungryong Kim<sup>&dagger;</sup></a>, <a>Yang Zhou<sup>&dagger;</sup></a>
	 		</h3>
		  	<h3 class="sectionContentSubTitle"><em> CVPR 2025
	 		</em></h3>
    	</section>
		<aside class="externalResourcesNav">

		  <div class="dropdown"><span></span><a href="https://cvlab-kaist.github.io/Visual-Persona/" target="_blank"><b>Project Page</b></a></div>
		  <div class="dropdown"> <span><b>Abstract</b></span>
			<div class="dropdown-content">
			  <p style="text-align:left;">We introduce Visual Persona, a foundation model for text-to-image full-body human customization that, given a single in-the-wild human image, generates diverse images of the individual guided by text descriptions. Unlike prior methods that focus solely on preserving facial identity, our approach captures detailed full-body appearance, aligning with text descriptions for body structure and scene variations. Training this model requires large-scale paired human data, consisting of multiple images per individual with consistent full-body identities, which is notoriously difficult to obtain. To address this, we propose a data curation pipeline leveraging vision-language models to evaluate full-body appearance consistency, resulting in Visual Persona-500K, a dataset of 580k paired human images across 100k unique identities. For precise appearance transfer, we introduce a transformer encoder-decoder architecture adapted to a pre-trained text-to-image diffusion model, which augments the input image into distinct body regions, encodes these regions as local appearance features, and projects them into dense identity embeddings independently to condition the diffusion model for synthesizing customized images. Visual Persona consistently surpasses existing approaches, generating high-quality, customized images from in-the-wild inputs. Extensive ablation studies validate design choices, and we demonstrate the versatility of Visual Persona across various downstream tasks.		  </p></div>
		  </div>
		  <div class="dropdown"><span></span><a href="https://arxiv.org/abs/2503.15406" target="_blank"><b>Paper</b></a></div>
		  <div class="dropdown"><span></span><a href="https://github.com/cvlab-kaist/Visual-Persona" target="_blank"><b>Code</b></a></div>
		</aside>

	  <br><br>

	  <!-- AM-Adapter  -->
	  	<div class="sectionContent">
<img src="AboutPageAssets/teasers/amadapter_teaser.png" alt="AM Adapter Teaser Image" width="100%">
	  	</div>
	  	<section class="section2Content">
      		<h2 class="sectionContentTitle"> AM-Adapter: Appearance Matching Adapter
				for Exemplar-based Semantic Image Synthesis in-the-Wild</h2>
      		<h3 class="sectionContentSubTitle"><a>Siyoon Jin</a>, <b>Jisu Nam</b>, <a>Jiyoung Kim</a>, <a>Dahyun Chung</a>, <a>Yeong-Seok Kim</a>, <a>Joonhyung Park</a>, <a>Heonjeong Chu</a>, <a>Seungryong Kim<sup>&dagger;</sup></a>
	 		</h3>
		  	<h3 class="sectionContentSubTitle"><em> arXiv preprint
	 		</em></h3>
    	</section>
		<aside class="externalResourcesNav">

		  <div class="dropdown"><span></span><a href="https://cvlab-kaist.github.io/AM-Adapter/" target="_blank"><b>Project Page</b></a></div>
		  <div class="dropdown"> <span><b>Abstract</b></span>
			<div class="dropdown-content">
			  <p style="text-align:left;">Exemplar-based semantic image synthesis generates images aligned with semantic content while preserving the appearance of an exemplar. Conventional structure-guidance models like ControlNet, are limited as they rely solely on text prompts to control appearance and cannot utilize exemplar images as input. Recent tuning-free approaches address this by transferring local appearance via implicit cross-image matching in the augmented self-attention mechanism of pre-trained diffusion models. However, prior works are often restricted to single-object cases or foreground object appearance transfer, struggling with complex scenes involving multiple objects. To overcome this, we propose AM-Adapter (Appearance Matching Adapter) to address exemplar-based semantic image synthesis in-the-wild, enabling multi-object appearance transfer from a single scene-level image. AM-Adapter automatically transfers local appearances from the scene-level input. AM-Adapter alternatively provides controllability to map user-defined object details to specific locations in the synthesized images. Our learnable framework enhances cross-image matching within augmented self-attention by integrating semantic information from segmentation maps. To disentangle generation and matching, we adopt stage-wise training. We first train the structure-guidance and generation networks, followed by training the matching adapter while keeping the others frozen. During inference, we introduce an automated exemplar retrieval method for selecting exemplar image-segmentation pairs efficiently. Despite utilizing minimal learnable parameters, AM-Adapter achieves state-of-the-art performance, excelling in both semantic alignment and local appearance fidelity. Extensive ablations validate our design choices.		  </p></div>
		  </div>
		  <div class="dropdown"><span></span><a href="https://arxiv.org/abs/2412.03150" target="_blank"><b>Paper</b></a></div>
		  <div class="dropdown"><span></span><a href="https://github.com/cvlab-kaist/AM-Adapter" target="_blank"><b>Code</b></a></div>
		</aside>

	  <br><br>

	<!-- LocoTrack  -->
	  	<div class="sectionContent">
<img src="AboutPageAssets/teasers/locotrack_teaser.png" alt="Locotrack Teaser Image" width="100%">
	  	</div>
	  	<section class="section2Content">
      		<h2 class="sectionContentTitle"> Local All-Pair Correspondence for Point Tracking</h2>
      		<h3 class="sectionContentSubTitle"><a>Seokju Cho</a>, <a>Jiahui Huang</a>, <b>Jisu Nam</b>, <a>Honggyu An</a>, <a>Seungryong Kim<sup>&dagger;</sup></a>, <a>Joon-Young Lee<sup>&dagger;</sup></a>
	 		</h3>
		  	<h3 class="sectionContentSubTitle"><em> ECCV 2024
	 		</em></h3>
    	</section>
		<aside class="externalResourcesNav">

		  <div class="dropdown"><span></span><a href="https://ku-cvlab.github.io/locotrack/" target="_blank"><b>Project Page</b></a></div>
		  <div class="dropdown"> <span><b>Abstract</b></span>
			<div class="dropdown-content">
			  <p style="text-align:left;">We introduce LocoTrack, a highly accurate and efficient model designed for the task of tracking any point (TAP) across video sequences. Previous approaches in this task often rely on local 2D correlation maps to establish correspondences from a point in the query image to a local region in the target image, which often struggle with homogeneous regions or repetitive features, leading to matching ambiguities. LocoTrack overcomes this challenge with a novel approach that utilizes all-pair correspondences across regions, i.e., local 4D correlation, to establish precise correspondences, with bidirectional correspondence and matching smoothness significantly enhancing robustness against ambiguities. We also incorporate a lightweight correlation encoder to enhance computational efficiency, and a compact Transformer architecture to integrate long-term temporal information. LocoTrack achieves unmatched accuracy on all TAP-Vid benchmarks and operates at a speed almost 6 times faster than the current state-of-the-art.			  </p></div>
		  </div>
		  <div class="dropdown"><span></span><a href="https://arxiv.org/abs/2407.15420" target="_blank"><b>Paper</b></a></div>
		  <div class="dropdown"><span></span><a href="https://github.com/KU-CVLAB/LocoTrack" target="_blank"><b>Code</b></a></div>
		</aside>

	  <br><br>

	<!-- DreamMatcher  -->
	  	<div class="sectionContent">
<img src="AboutPageAssets/teasers/dreammatcher_teaser.png" alt="DreamMatcher Teaser Image" width="100%">
	  	</div>
	  	<section class="section2Content">
      		<h2 class="sectionContentTitle"> DreamMatcher: Appearance Matching Self-Attention for Semantically-Consistent Text-to-Image Personalization</h2>
      		<h3 class="sectionContentSubTitle"><b>Jisu Nam</b>, <a>Heesu Kim</a>, <a>DongJae Lee</a>, <a>Siyoon Jin</a>, <a>Seungryong Kim<sup>&dagger;</sup></a>, <a>Seunggyu Chang<sup>&dagger;</sup></a> 
	 		</h3>
		  	<h3 class="sectionContentSubTitle"><em> CVPR 2024
	 		</em></h3>
    	</section>
		<aside class="externalResourcesNav">

		  <div class="dropdown"><span></span><a href="https://ku-cvlab.github.io/DreamMatcher/" target="_blank"><b>Project Page</b></a></div>
		  <div class="dropdown"> <span><b>Abstract</b></span>
			<div class="dropdown-content">
			  <p style="text-align:left;">The objective of Text-to-Image (T2I) personalization is to customize a diffusion model to a user-provided reference concept, generating diverse images of the concept aligned with the target prompts. Conventional methods for T2I personalization represent the reference concepts using unique text embeddings. However, they often fail to accurately mimic the appearance of the reference. To address this, one solution may be explicitly conditioning the reference images into the target denoising process, called key-value replacement. However, prior works are constrained to local editing since they disrupt the structure path of the pre-trained T2I model, leading to suboptimal matching. To overcome this, we propose a novel plug-in method, DreamMatcher, which reformulates T2I personalization as semantic matching. Specifically, DreamMatcher replaces the target values with reference values aligned by semantic matching, while leaving the structure path unchanged to preserve the versatile capability of pre-trained T2I models for generating diverse structures. We also introduce a semantic-consistent masking strategy to isolate the personalized concept from irrelevant regions, such as the background or objects newly introduced by the target prompts. DreamMatcher is compatible with any existing T2I models, showing significant improvements in complex personalization scenarios. Intensive experiments and analyses demonstrate the effectiveness of our approach.			  </p></div>
		  </div>
		  <div class="dropdown"><span></span><a href="https://arxiv.org/abs/2402.09812" target="_blank"><b>Paper</b></a></div>
		  <div class="dropdown"><span></span><a href="https://github.com/KU-CVLAB/DreamMatcher" target="_blank"><b>Code</b></a></div>
		</aside>

	  <br><br>

	  <!-- DiffMatch  -->
	  	<div class="sectionContent">
<img src="AboutPageAssets/teasers/diffmatch_teaser.png" alt="DiffMatch Teaser Image" width="100%">
	  	</div>
	  	<section class="section2Content">
      		<h2 class="sectionContentTitle"> Diffusion Model for Dense Matching</h2>
      		<h3 class="sectionContentSubTitle"><b>Jisu Nam</b>, <a>Gyuseong Lee</a>, <a>Sunwoo Kim</a>, <a>Hyeonsu Kim</a>, <a>Hyoungwon Cho</a>, <a>Seyeon Kim</a>, <a>Seungryong
Kim</a>  
	 		</h3>
		  	<h3 class="sectionContentSubTitle"><em>ICLR 2024, <b style="color:red">Oral (1.2% acceptance rate)</b>
	 		</em></h3>
    	</section>
		<aside class="externalResourcesNav">

		  <div class="dropdown"><span></span><a href="https://ku-cvlab.github.io/DiffMatch/" target="_blank"><b>Project Page</b></a></div>
		  <div class="dropdown"> <span><b>Abstract</b></span>
			<div class="dropdown-content">
			  <p style="text-align:left;">The objective for establishing dense correspondence between paired images conists of two terms: a data term and a prior term. While conventional techniques focused on defining hand-designed prior terms, which are difficult to formulate, recent approaches have focused on learning the data term with deep neural net- works without explicitly modeling the prior, assuming that the model itself has the capacity to learn an optimal prior from a large-scale dataset. The performance improvement was obvious, however, they often fail to address inherent ambiguities of matching, such as textureless regions, repetitive patterns, large displacements, or noises. To address this, we propose DiffMatch, a novel conditional diffusion-based framework designed to explicitly model both the data and prior terms for dense matching. This is accomplished by leveraging a conditional denoising diffusion model that explicitly takes matching cost and injects the prior within generative process. However, limited resolution of the diffusion model is a major hindrance. We address this with a cascaded pipeline, starting with a low-resolution model, followed by a super-resolution model that successively upsamples and incorporates finer details to the matching field. Our experimental results demonstrate signifi- cant performance improvements of our method over existing approaches, and the ablation studies validate our design choices along with the effectiveness of each component. The code and pretrained weights will be available.			  </p></div>
		  </div>
		  <div class="dropdown"><span></span><a href="https://openreview.net/pdf/969f922ad6124503cc67cacd76aa7e7b928a7882.pdf" target="_blank"><b>Paper</b></a></div>
		  <div class="dropdown"><span></span><a href="https://github.com/KU-CVLAB/DiffMatch" target="_blank"><b>Code</b></a></div>
		</aside>

	  <br><br>

    	<!-- NeMF  -->
	  	<div class="sectionContent">
		<img src="AboutPageAssets/teasers/nemf_teaser.png" alt="nemf Teaser Image" width="100%">
	  	</div>
	  	<section class="section2Content">
      		<h2 class="sectionContentTitle"> Neural Matching Fields: Implicit Representation of Matching Fields for Visual Correspondence</h2>
      		<h3 class="sectionContentSubTitle">Sunghwan Hong, <b>Jisu Nam</b>, Seokju Cho, Susung Hong, Sangryul Jeon, Dongbo Min, and Seungryong
Kim  </h3>
		  	<h3 class="sectionContentSubTitle"><em> NeurIPS 2022
	 		</em></h3>
    	</section>
		<aside class="externalResourcesNav"> 
		
		  <div class="dropdown"><span></span><a href="https://ku-cvlab.github.io/NeMF/" target="_blank"><b>Project Page</b></a></div>
		  <div class="dropdown"> <span><b>Abstract</b></span>
			<div class="dropdown-content">
			  <p style="text-align:left;">
				Existing pipelines of semantic correspondence commonly include extracting high level semantic features for the invariance against intra-class variations and background clutters. This architecture, however, inevitably results in a low-resolution matching field that additionally requires an ad-hoc interpolation process as a post-processing for converting it into a high-resolution one, certainly limiting the performance of matching results. To overcome this, inspired by recent success of implicit neural representation, we present a novel method for semantic correspondence, called neural matching field (NeMF). However, complicacy and high-dimensionality of a 4D matching field are the major hindrances. To address them, we propose a cost embedding network consisting of convolution and self-attention layers to process the coarse cost volume to obtain cost feature representation, which is used as a guidance for establishing high-precision matching field through the following fully-connected network. Although this may help to better structure the matching field, learning a high-dimensional matching field remains challenging mainly due to computational complexity, since a naïve ex- haustive inference would require querying from all pixels in the 4D space to infer pixel-wise correspondences. To overcome this, in the training phase, we randomly sample matching candidates. In the inference phase, we propose a novel inference approach which iteratively performs PatchMatch-based inference and coordinate optimization at test time. With the proposed method, competitive results are at- tained on several standard benchmarks for semantic correspondence.		  </p>
			</div>
		  </div>
		  <div class="dropdown"><span></span><a href="https://arxiv.org/abs/2210.02689" target="_blank"><b>Paper</b></a></div>
		  <div class="dropdown"><span></span><a href="https://github.com/KU-CVLAB/NeMF" target="_blank"><b>Code</b></a></div>
		</aside>
	  
		<br><br>
		
	  <!--VAT  -->
	  <div class="sectionContent">
		<img src="AboutPageAssets/teasers/vat_teaser.png" alt="vat Teaser Image"  width="100%">
	  	</div>
	  	<section class="section2Content">
      		<h2 class="sectionContentTitle"> Cost Aggregation with 4D Convolutional Swin Transformer for Few-Shot Segmentation</h2>
			  <h3 class="sectionContentSubTitle">Sunghwan Hong*, Seokju Cho*, <b>Jisu Nam</b>, Stephen Lin, Seungryong Kim</h3>
		  	<h3 class="sectionContentSubTitle"><em>ECCV 2022
	 		</em></h3>
    	</section>
		<aside class="externalResourcesNav">

		  <div class="dropdown"><span></span><a href="https://seokju-cho.github.io/VAT/" target="_blank"><b>Project Page</b></a></div>
		  <div class="dropdown"> <span><b>Abstract</b></span>
			<div class="dropdown-content">
			  <p style="text-align:left;">
 			 This paper presents a novel cost aggregation network, called Volumetric Aggregation with Transformers (VAT), for few-shot segmentation. The use of transformers can benefit correlation map aggregation through self-attention over a global receptive field. However, the tokenization of a correlation map for transformer processing can be detrimental, because the discontinuity at token boundaries reduces the local context available near the token edges and decreases inductive bias. To address this problem, we propose a 4D Convolutional Swin Transformer, where a high-dimensional Swin Transformer is preceded by a series of small-kernel convolutions that impart local context to all pixels and introduce convolutional inductive bias. We additionally boost aggregation performance by applying transformers within a pyramidal structure, where aggregation at a coarser level guides aggregation at a finer level. Noise in the transformer output is then filtered in the subsequent decoder with the help of the query’s appearance embedding. With this model, a new state-of-the-art is set for all the standard benchmarks in few-shot segmentation. It is shown that VAT attains state-of-the-art performance for semantic correspondence as well, where cost aggregation also plays a central role.
			  </p>
			</div>
		  </div>
		  <div class="dropdown"><span></span><a href="https://arxiv.org/abs/2207.10866" target="_blank"><b>Paper</b></a></div>
		  <div class="dropdown"><span></span><a href="https://github.com/Seokju-Cho/Volumetric-Aggregation-Transformer" target="_blank"><b>Code</b></a></div>
		</aside>
	  
		<br><br><br>

	  <section class="section3">
	<h2 class="sectionTitle">Honors</h2>
	<hr class="sectionRule">
	<b>37th Workshop on Image Processing and Image Understanding</b>, 2025 <br>
	Best Paper Award <br>

	<br>

	<b>37th Workshop on Image Processing and Image Understanding</b>, 2025 <br>
	Best Poster Presentation Award <br>

	<br>

	<b>Qualcomm Innovation Fellowship</b>, 2024 <br>
	Winner <br>
	<br>

	<b>33th Artificial Intelligence and Signal Processing</b>, 2023 <br>
	Best Paper Award
	<br>
	<br>

  <p class="footerDisclaimer">Template of <a href="https://lioryariv.github.io/">Lior Yariv</a><span></span></p>
  <p class="footerNote"></p>
</footer>
</body>
</html>
